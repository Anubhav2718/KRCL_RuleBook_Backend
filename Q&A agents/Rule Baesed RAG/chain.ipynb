{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fc1aa46",
   "metadata": {},
   "source": [
    "# Retrival and Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c30014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "loader = UnstructuredPDFLoader(\"KRCL G & SR 2020.pdf\", mode=\"elements\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acefa36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import re\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "def process_legal_documents(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"Process loaded PDF documents into structured legal chunks with reference extraction.\"\"\"\n",
    "    \n",
    "    # Step 1: Group by rule number with enhanced pattern matching\n",
    "    def group_by_rule(docs: List[Document]) -> List[Document]:\n",
    "        # Pattern matches all rule types:\n",
    "        # - S.R.2.24, G.R.3.15\n",
    "        # - Standard rules: 2.03, 3.01\n",
    "        # - Rule 4.05, Article 5.02, ยง6.01\n",
    "        rule_pattern = re.compile(\n",
    "            r\"^(?:(?:S\\.R\\.|G\\.R\\.|Rule|Article|Section|ยง)?\\s*)?\"\n",
    "            r\"((?:[SG]\\.R\\.)?\\d{1,2}\\.\\d{2}(?:\\.\\d+)?|\\d{1,2}\\-\\d{2})\", \n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        grouped = []\n",
    "        current_rule = None\n",
    "        current_metadata = {}\n",
    "        buffer = []\n",
    "\n",
    "        for doc in docs:\n",
    "            text = doc.page_content.strip()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            if not current_metadata:\n",
    "                current_metadata = doc.metadata.copy()\n",
    "\n",
    "            match = rule_pattern.match(text)\n",
    "            if match:\n",
    "                if current_rule and buffer:\n",
    "                    grouped.append(create_rule_document(buffer, current_rule, current_metadata))\n",
    "                current_rule = match.group(1)\n",
    "                # Normalize rule format\n",
    "                current_rule = current_rule.replace(' ', '')  # Remove spaces in S.R./G.R.\n",
    "                buffer = [text]\n",
    "                current_metadata = doc.metadata.copy()\n",
    "            else:\n",
    "                buffer.append(text)\n",
    "\n",
    "        if current_rule and buffer:\n",
    "            grouped.append(create_rule_document(buffer, current_rule, current_metadata))\n",
    "\n",
    "        return grouped\n",
    "\n",
    "    def create_rule_document(content: List[str], rule: str, metadata: dict) -> Document:\n",
    "        \"\"\"Create a rule document with proper metadata.\"\"\"\n",
    "        rule_type = \"SR\" if rule.startswith('S.R.') else \\\n",
    "                   \"GR\" if rule.startswith('G.R.') else \"Standard\"\n",
    "        \n",
    "        metadata = metadata.copy()\n",
    "        metadata.update({\n",
    "            \"rule\": rule,\n",
    "            \"rule_type\": rule_type,\n",
    "            \"source\": metadata.get(\"source\", \"KRCL G & SR 2020.pdf\"),\n",
    "            \"document_type\": \"legal_rule\"\n",
    "        })\n",
    "        return Document(\n",
    "            page_content=\"\\n\".join(content),\n",
    "            metadata=metadata\n",
    "        )\n",
    "\n",
    "    # Step 2: Group the documents by rule\n",
    "    grouped_docs = group_by_rule(documents)\n",
    "\n",
    "    # Step 3: Merge small rules intelligently\n",
    "    def merge_small_rules(docs: List[Document], min_length: int = 100) -> List[Document]:\n",
    "        merged = []\n",
    "        previous_doc = None\n",
    "\n",
    "        for i, doc in enumerate(docs):\n",
    "            # Handle first document case\n",
    "            if i == 0 and len(doc.page_content) < min_length:\n",
    "                if len(docs) > 1 and docs[i+1].metadata.get('rule_type') == doc.metadata.get('rule_type'):\n",
    "                    # Merge forward with next document\n",
    "                    docs[i+1].page_content = doc.page_content + \"\\n\\n\" + docs[i+1].page_content\n",
    "                    docs[i+1].metadata[\"combined_rules\"] = (\n",
    "                        doc.metadata[\"rule\"] + \"; \" + \n",
    "                        docs[i+1].metadata.get(\"combined_rules\", docs[i+1].metadata[\"rule\"])\n",
    "                    )\n",
    "                    continue\n",
    "                merged.append(doc)\n",
    "                continue\n",
    "\n",
    "            if previous_doc and len(doc.page_content) < min_length:\n",
    "                # Only merge if same rule type\n",
    "                if previous_doc.metadata.get('rule_type') == doc.metadata.get('rule_type'):\n",
    "                    previous_doc.page_content += \"\\n\\n\" + doc.page_content\n",
    "                    previous_doc.metadata[\"combined_rules\"] = (\n",
    "                        previous_doc.metadata.get(\"combined_rules\", previous_doc.metadata[\"rule\"]) + \n",
    "                        f\"; {doc.metadata['rule']}\"\n",
    "                    )\n",
    "                    continue\n",
    "                else:\n",
    "                    merged.append(previous_doc)\n",
    "                    previous_doc = doc\n",
    "                    continue\n",
    "            else:\n",
    "                if previous_doc:\n",
    "                    merged.append(previous_doc)\n",
    "                previous_doc = doc\n",
    "\n",
    "        if previous_doc:\n",
    "            merged.append(previous_doc)\n",
    "\n",
    "        return merged\n",
    "\n",
    "    merged_docs = merge_small_rules(grouped_docs)\n",
    "\n",
    "    # Step 4: Split long rules with metadata preservation and reference extraction\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=150,\n",
    "        length_function=len,\n",
    "        keep_separator=True\n",
    "    )\n",
    "\n",
    "    final_documents = []\n",
    "    for doc in merged_docs:\n",
    "        try:\n",
    "            splits = splitter.split_documents([doc])\n",
    "            \n",
    "            # Extract references from each split\n",
    "            reference_pattern = re.compile(\n",
    "                r\"\\b(S\\.R\\.\\d{1,2}\\.\\d{2}(?:\\.\\d+)?)\\s+of\\s+(G\\.R\\.\\d{1,2}\\.\\d{2}(?:\\.\\d+)?)\\b\"\n",
    "            )\n",
    "            \n",
    "            for chunk_idx, split in enumerate(splits, 1):\n",
    "                # Extract references\n",
    "                references = reference_pattern.findall(split.page_content)\n",
    "                if references:\n",
    "                    split.metadata[\"references\"] = [f\"{sr} of {gr}\" for sr, gr in references]\n",
    "                \n",
    "                # Merge metadata with priority to split-specific values\n",
    "                split.metadata = {\n",
    "                    **doc.metadata,\n",
    "                    **split.metadata,  # Preserves extracted references\n",
    "                    \"chunk_id\": f\"{doc.metadata['rule']}_chunk_{chunk_idx}\",\n",
    "                    \"total_chunks\": len(splits),\n",
    "                    \"chunk_number\": chunk_idx\n",
    "                }\n",
    "            \n",
    "            final_documents.extend(splits)\n",
    "        except Exception as e:\n",
    "            print(f\"Error splitting rule {doc.metadata['rule']}: {str(e)}\")\n",
    "            doc.metadata.update({\n",
    "                \"chunk_id\": f\"{doc.metadata['rule']}_chunk_1\",\n",
    "                \"total_chunks\": 1,\n",
    "                \"chunk_number\": 1,\n",
    "                \"references\": extract_references(doc.page_content)  # Extract even if not split\n",
    "            })\n",
    "            final_documents.append(doc)\n",
    "\n",
    "    return final_documents\n",
    "\n",
    "def extract_references(text: str) -> List[str]:\n",
    "    \"\"\"Extract cross-references from text.\"\"\"\n",
    "    reference_pattern = re.compile(\n",
    "        r\"\\b(S\\.R\\.\\d{1,2}\\.\\d{2}(?:\\.\\d+)?)\\s+of\\s+(G\\.R\\.\\d{1,2}\\.\\d{2}(?:\\.\\d+)?)\\b\"\n",
    "    )\n",
    "    references = reference_pattern.findall(text)\n",
    "    return [f\"{sr} of {gr}\" for sr, gr in references]\n",
    "\n",
    "# Process documents\n",
    "documents = process_legal_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b923185a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swanu\\AppData\\Local\\Temp\\ipykernel_19760\\3623215180.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "306a671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ffebe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local(\"faiss_index_krcl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17586d32",
   "metadata": {},
   "source": [
    "### Loading the database later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e27d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings  # or whichever embeddings you used\n",
    "\n",
    "# Initialize your embeddings (must match what you used to create the index)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")  # example\n",
    "\n",
    "# Load the FAISS index with safety override\n",
    "db = FAISS.load_local(\n",
    "    \"faiss_index_krcl\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True  # Required for loading pickle files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af0708af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.13\n",
      "LIMIT OF SPEED WITH ENGINE TENDER FOREMOST:-\n",
      "(1)\n",
      "(a) A passenger train or a mixed train shall not be drawn outside station limit by a steam engine running tender foremost, except-\n",
      "(I)\n",
      "under a written order issued by the authorized officer:- or\n",
      "(II)\n",
      "in a case of unavoidable necessity, to be established by the Loco Pilot.\n",
      "(b) When any such train is so drawn, the speed shall not exceed 25 kilometers and hour, or such higher speed, not exceeding 40 kilometers an hour, as may be authorised by approved special instructions.\n",
      "(2)\n",
      "in case of unavoidable necessity, goods trains may run with steam engins tender foremost at a speed not exceeding 25 kilometers an hour or such higher speed, which shall, In no Circumstances, exceed 40 kilometres an hour, as may be laid down by special instructions.\n"
     ]
    }
   ],
   "source": [
    "query = \"what is LIMIT OF SPEED WITH ENGINE TENDER FOREMOST\"\n",
    "retireved_results=db.similarity_search(query)\n",
    "print(retireved_results[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b98a8e",
   "metadata": {},
   "source": [
    "## Models Using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "190e8dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swanu\\AppData\\Local\\Temp\\ipykernel_19760\\3889164935.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"mistral\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ollama(model='mistral')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"mistral\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ded1839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert assistant answering questions about railway operations and safety rules from the *KRCL General and Subsidiary Rules 2020* manual.\n",
    "\n",
    "Your task is to provide accurate, section-based answers **only** from the provided context. Do not guess. Do not invent rules. If the answer cannot be found in the context, clearly state: \"The information is not available in the provided rules.\"\n",
    "\n",
    "Warning: Your response may be used by railway staff in real operational scenarios. Wrong or misleading answers may lead to **accidents or disciplinary action**. Respond carefully and precisely.\n",
    "\n",
    "Instructions:\n",
    "- Identify the correct rule number and section from the context.\n",
    "- Provide the **exact wording** or an accurate summary.\n",
    "- Format the rule reference like this: *Rule 2.03 โ Knowledge of Rules*.\n",
    "- If multiple rules apply, cite each.\n",
    "- If the answer is not in the context, **say so clearly**.\n",
    "\n",
    "I'll tip you $1000 dollars everytime you give correct answer.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Answer (with rule reference):\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66106ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c850b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000020C3FBF19D0>, search_kwargs={})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Retrievers: A retriever is an interface that returns documents given\n",
    " an unstructured query. It is more general than a vector store.\n",
    " A retriever does not need to be able to store documents, only to \n",
    " return (or retrieve) them. Vector stores can be used as the backbone\n",
    " of a retriever, but there are other types of retrievers as well. \n",
    " https://python.langchain.com/docs/modules/data_connection/retrievers/   \n",
    "\"\"\"\n",
    "\n",
    "retriever=db.as_retriever()\n",
    "retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fd9ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieval chain:This chain takes in a user inquiry, which is then\n",
    "passed to the retriever to fetch relevant documents. Those documents \n",
    "(and original inputs) are then passed to an LLM to generate a response\n",
    "https://python.langchain.com/docs/modules/chains/\n",
    "\"\"\"\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"HEAD LIGHT, MARKER LIGHTS AND SPEEDOMETER\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5031de8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" *Rule 4.14(1) โ A train shall not be worked at night or in thick, foggy or tempestous weather impairing visibility or in long tunnels, unless the engine carries an electric head light of an approved design and, in addition, two oil or electric white marker lights.*\\n*Rule 4.14(3) โ The electric head light on the engine shall be fitted with a switch to dim the light and shall be dimmed when the train remains stationary at a station.*\\n*Rule A.6 from SR 4.65(3) โ A Track Maintenance Machine must be equipped with prescribed head light and tail light, marker light and flasher light as per GR GR4.14 to 4.16 and SR's thereto.*\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da95a7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
