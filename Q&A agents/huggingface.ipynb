{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'C:\\\\Users\\\\swanu\\\\anaconda3\\\\Scripts\\\\RAG_GnSR\\\\GnSR and Manual\\\\KRCL G & SR 2020.pdf', 'coordinates': {'points': ((15.0, 164.29199284452147), (15.0, 197.29004359452148), (161.778320230542, 197.29004359452148), (161.778320230542, 164.29199284452147)), 'system': 'PixelSpace', 'layout_width': 510.0, 'layout_height': 680.0}, 'file_directory': 'C:\\\\Users\\\\swanu\\\\anaconda3\\\\Scripts\\\\RAG_GnSR\\\\GnSR and Manual', 'filename': 'KRCL G & SR 2020.pdf', 'languages': ['eng'], 'last_modified': '2025-06-24T23:09:43', 'page_number': 6, 'filetype': 'application/pdf', 'category': 'Title', 'element_id': '6c056a3a6296695a75b3afd832abdfa3', 'rule': '1.01', 'rule_type': 'Standard', 'document_type': 'legal_rule', 'chunk_id': '1.01_chunk_1', 'total_chunks': 1, 'chunk_number': 1}, page_content='1.01 1.02 Definitions 1.03 Classification of stations\\nShort title and commencement\\nCHAPTER II\\nRULES APPLYING TO RAILWAY SERVANTS GENERALLY')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read the ppdfs from the folder\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "\n",
    "loader = UnstructuredPDFLoader(r\"C:\\Users\\swanu\\anaconda3\\Scripts\\RAG_GnSR\\GnSR and Manual\\KRCL G & SR 2020.pdf\",mode=\"elements\")\n",
    "docs = loader.load()\n",
    "\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import re\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "def process_legal_documents(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"Process loaded PDF documents into structured legal chunks with reference extraction.\"\"\"\n",
    "    \n",
    "    # Step 1: Group by rule number with enhanced pattern matching\n",
    "    def group_by_rule(docs: List[Document]) -> List[Document]:\n",
    "        # Pattern matches all rule types:\n",
    "        # - S.R.2.24, G.R.3.15\n",
    "        # - Standard rules: 2.03, 3.01\n",
    "        # - Rule 4.05, Article 5.02, ยง6.01\n",
    "        rule_pattern = re.compile(\n",
    "            r\"^(?:(?:S\\.R\\.|G\\.R\\.|Rule|Article|Section|ยง)?\\s*)?\"\n",
    "            r\"((?:[SG]\\.R\\.)?\\d{1,2}\\.\\d{2}(?:\\.\\d+)?|\\d{1,2}\\-\\d{2})\", \n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        grouped = []\n",
    "        current_rule = None\n",
    "        current_metadata = {}\n",
    "        buffer = []\n",
    "\n",
    "        for doc in docs:\n",
    "            text = doc.page_content.strip()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            if not current_metadata:\n",
    "                current_metadata = doc.metadata.copy()\n",
    "\n",
    "            match = rule_pattern.match(text)\n",
    "            if match:\n",
    "                if current_rule and buffer:\n",
    "                    grouped.append(create_rule_document(buffer, current_rule, current_metadata))\n",
    "                current_rule = match.group(1)\n",
    "                # Normalize rule format\n",
    "                current_rule = current_rule.replace(' ', '')  # Remove spaces in S.R./G.R.\n",
    "                buffer = [text]\n",
    "                current_metadata = doc.metadata.copy()\n",
    "            else:\n",
    "                buffer.append(text)\n",
    "\n",
    "        if current_rule and buffer:\n",
    "            grouped.append(create_rule_document(buffer, current_rule, current_metadata))\n",
    "\n",
    "        return grouped\n",
    "\n",
    "    def create_rule_document(content: List[str], rule: str, metadata: dict) -> Document:\n",
    "        \"\"\"Create a rule document with proper metadata.\"\"\"\n",
    "        rule_type = \"SR\" if rule.startswith('S.R.') else \\\n",
    "                   \"GR\" if rule.startswith('G.R.') else \"Standard\"\n",
    "        \n",
    "        metadata = metadata.copy()\n",
    "        metadata.update({\n",
    "            \"rule\": rule,\n",
    "            \"rule_type\": rule_type,\n",
    "            \"source\": metadata.get(\"source\", \"KRCL G & SR 2020.pdf\"),\n",
    "            \"document_type\": \"legal_rule\"\n",
    "        })\n",
    "        return Document(\n",
    "            page_content=\"\\n\".join(content),\n",
    "            metadata=metadata\n",
    "        )\n",
    "\n",
    "    # Step 2: Group the documents by rule\n",
    "    grouped_docs = group_by_rule(documents)\n",
    "\n",
    "    # Step 3: Merge small rules intelligently\n",
    "    def merge_small_rules(docs: List[Document], min_length: int = 100) -> List[Document]:\n",
    "        merged = []\n",
    "        previous_doc = None\n",
    "\n",
    "        for i, doc in enumerate(docs):\n",
    "            # Handle first document case\n",
    "            if i == 0 and len(doc.page_content) < min_length:\n",
    "                if len(docs) > 1 and docs[i+1].metadata.get('rule_type') == doc.metadata.get('rule_type'):\n",
    "                    # Merge forward with next document\n",
    "                    docs[i+1].page_content = doc.page_content + \"\\n\\n\" + docs[i+1].page_content\n",
    "                    docs[i+1].metadata[\"combined_rules\"] = (\n",
    "                        doc.metadata[\"rule\"] + \"; \" + \n",
    "                        docs[i+1].metadata.get(\"combined_rules\", docs[i+1].metadata[\"rule\"])\n",
    "                    )\n",
    "                    continue\n",
    "                merged.append(doc)\n",
    "                continue\n",
    "\n",
    "            if previous_doc and len(doc.page_content) < min_length:\n",
    "                # Only merge if same rule type\n",
    "                if previous_doc.metadata.get('rule_type') == doc.metadata.get('rule_type'):\n",
    "                    previous_doc.page_content += \"\\n\\n\" + doc.page_content\n",
    "                    previous_doc.metadata[\"combined_rules\"] = (\n",
    "                        previous_doc.metadata.get(\"combined_rules\", previous_doc.metadata[\"rule\"]) + \n",
    "                        f\"; {doc.metadata['rule']}\"\n",
    "                    )\n",
    "                    continue\n",
    "                else:\n",
    "                    merged.append(previous_doc)\n",
    "                    previous_doc = doc\n",
    "                    continue\n",
    "            else:\n",
    "                if previous_doc:\n",
    "                    merged.append(previous_doc)\n",
    "                previous_doc = doc\n",
    "\n",
    "        if previous_doc:\n",
    "            merged.append(previous_doc)\n",
    "\n",
    "        return merged\n",
    "\n",
    "    merged_docs = merge_small_rules(grouped_docs)\n",
    "\n",
    "    # Step 4: Split long rules with metadata preservation and reference extraction\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=150,\n",
    "        length_function=len,\n",
    "        keep_separator=True\n",
    "    )\n",
    "\n",
    "    final_documents = []\n",
    "    for doc in merged_docs:\n",
    "        try:\n",
    "            splits = splitter.split_documents([doc])\n",
    "            \n",
    "            # Extract references from each split\n",
    "            reference_pattern = re.compile(\n",
    "                r\"\\b(S\\.R\\.\\d{1,2}\\.\\d{2}(?:\\.\\d+)?)\\s+of\\s+(G\\.R\\.\\d{1,2}\\.\\d{2}(?:\\.\\d+)?)\\b\"\n",
    "            )\n",
    "            \n",
    "            for chunk_idx, split in enumerate(splits, 1):\n",
    "                # Extract references\n",
    "                references = reference_pattern.findall(split.page_content)\n",
    "                if references:\n",
    "                    split.metadata[\"references\"] = [f\"{sr} of {gr}\" for sr, gr in references]\n",
    "                \n",
    "                # Merge metadata with priority to split-specific values\n",
    "                split.metadata = {\n",
    "                    **doc.metadata,\n",
    "                    **split.metadata,  # Preserves extracted references\n",
    "                    \"chunk_id\": f\"{doc.metadata['rule']}_chunk_{chunk_idx}\",\n",
    "                    \"total_chunks\": len(splits),\n",
    "                    \"chunk_number\": chunk_idx\n",
    "                }\n",
    "            \n",
    "            final_documents.extend(splits)\n",
    "        except Exception as e:\n",
    "            print(f\"Error splitting rule {doc.metadata['rule']}: {str(e)}\")\n",
    "            doc.metadata.update({\n",
    "                \"chunk_id\": f\"{doc.metadata['rule']}_chunk_1\",\n",
    "                \"total_chunks\": 1,\n",
    "                \"chunk_number\": 1,\n",
    "                \"references\": extract_references(doc.page_content)  # Extract even if not split\n",
    "            })\n",
    "            final_documents.append(doc)\n",
    "\n",
    "    return final_documents\n",
    "\n",
    "def extract_references(text: str) -> List[str]:\n",
    "    \"\"\"Extract cross-references from text.\"\"\"\n",
    "    reference_pattern = re.compile(\n",
    "        r\"\\b(S\\.R\\.\\d{1,2}\\.\\d{2}(?:\\.\\d+)?)\\s+of\\s+(G\\.R\\.\\d{1,2}\\.\\d{2}(?:\\.\\d+)?)\\b\"\n",
    "    )\n",
    "    references = reference_pattern.findall(text)\n",
    "    return [f\"{sr} of {gr}\" for sr, gr in references]\n",
    "\n",
    "# Process documents\n",
    "final_documents = process_legal_documents(docs)\n",
    "\n",
    "final_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1695"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding Using Huggingface\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "huggingface_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"},  # This enables GPU acceleration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.54984603e-02 -1.32178171e-02  9.28688720e-02  1.21875275e-02\n",
      " -2.52666008e-02  1.15616992e-01  5.61870774e-03  3.02802511e-02\n",
      " -1.37848243e-01 -5.20090945e-02 -4.14693728e-02  5.04869677e-04\n",
      "  3.97413708e-02 -6.86438382e-02 -4.11508232e-02 -3.02156173e-02\n",
      "  4.04950157e-02 -2.42434852e-02  3.48446928e-02  3.09494361e-02\n",
      "  7.56424367e-02  1.07497722e-02 -9.58742797e-02  4.69163731e-02\n",
      " -5.06225834e-03 -4.95175608e-02 -2.27279272e-02  9.62005407e-02\n",
      "  1.49883777e-02 -4.47839824e-03 -6.16348088e-02  5.39363101e-02\n",
      "  1.13636062e-01  5.06605543e-02  4.51007998e-03  4.06036153e-02\n",
      "  3.07764746e-02 -1.54581247e-02  2.92374920e-02  6.98646083e-02\n",
      " -4.87504564e-02 -9.59559903e-02 -6.63275942e-02  1.08579598e-01\n",
      "  6.43876418e-02  2.46755537e-02  1.74940974e-02 -9.55154225e-02\n",
      " -1.26703352e-01 -1.73531962e-03 -3.29455733e-02  2.66789533e-02\n",
      "  3.45032779e-03  9.94323716e-02 -4.55959961e-02 -1.46388300e-02\n",
      " -4.57849689e-02  8.48745089e-03  8.64446349e-03  8.00068304e-02\n",
      " -6.13888241e-02 -3.89190800e-02 -5.69189638e-02 -2.77795531e-02\n",
      "  8.97183493e-02 -3.67923602e-02 -6.12029471e-02  4.32763621e-02\n",
      "  5.74560240e-02  1.79712828e-02  8.70051235e-03  5.37277851e-03\n",
      " -3.11443117e-02  1.06487401e-01 -1.04363963e-01  4.01454017e-04\n",
      " -4.13442329e-02  9.75078642e-02 -4.47396189e-02 -8.40461478e-02\n",
      " -1.34722531e-01 -4.55779620e-02  4.05897684e-02 -4.92863841e-02\n",
      " -2.92928535e-02 -9.28712729e-03 -2.16549058e-02 -2.51369439e-02\n",
      " -6.72111809e-02  2.43538916e-02  2.95714997e-02 -5.42734042e-02\n",
      "  8.90289694e-02  5.08014783e-02 -4.88733687e-03  1.09914448e-02\n",
      " -7.10689873e-02 -6.48670178e-03  4.56114225e-02  2.58511547e-02\n",
      " -2.36981809e-02  3.90726924e-02 -9.03015062e-02  4.29089405e-02\n",
      " -4.12141569e-02 -3.40267606e-02 -2.96100210e-02 -4.79304977e-02\n",
      " -1.40094077e-02 -1.13612242e-01  1.46704121e-02  3.88669036e-03\n",
      " -6.46688640e-02 -5.68575561e-02 -4.63508293e-02  7.76358247e-02\n",
      " -2.35573743e-02 -8.44632369e-03  4.07003611e-02 -2.66018906e-03\n",
      " -3.01781353e-02 -1.36591066e-02  4.95604910e-02  2.19825078e-02\n",
      " -7.05017447e-02 -5.40544018e-02  6.34151101e-02  1.38957309e-33\n",
      " -5.92376664e-02 -1.87439565e-02 -2.47588772e-02  2.13854834e-02\n",
      "  3.69043536e-02  6.67997822e-03 -1.28072232e-01 -2.93382145e-02\n",
      "  2.06285040e-03  7.68350363e-02  2.95757148e-02  6.13026917e-02\n",
      "  1.00837192e-02 -3.01929433e-02  2.85801236e-02 -3.46236713e-02\n",
      "  5.09118289e-02  5.90356700e-02 -1.15971128e-02  3.40524577e-02\n",
      "  3.93130258e-02  4.67227288e-02  1.28908763e-02 -7.99269155e-02\n",
      "  6.41411170e-02  4.92983544e-03 -7.89935440e-02 -2.78468393e-02\n",
      "  6.51374925e-03  2.05039550e-02  3.93535681e-02 -2.80405357e-02\n",
      "  2.31943820e-02 -2.29740273e-02 -3.42816897e-02  4.91110049e-03\n",
      "  6.36524474e-03  3.07014082e-02 -2.69980412e-02 -9.00223851e-02\n",
      " -5.37974536e-02 -3.28494348e-02 -1.66382408e-03  4.17395271e-02\n",
      " -2.20302492e-03 -7.21889269e-03  1.99039429e-02 -1.32031376e-02\n",
      "  1.25630379e-01  7.52425194e-02 -4.75560408e-03 -4.85077947e-02\n",
      " -2.65526474e-02 -2.35704090e-02  3.16718444e-02  1.15894573e-02\n",
      " -2.56152302e-02  9.92157161e-02 -2.29866263e-02  3.79035361e-02\n",
      "  2.71301363e-02  8.82855356e-02 -1.18198767e-02  2.53679939e-02\n",
      "  3.35745700e-02 -8.48176852e-02  3.82559211e-03 -5.40693104e-02\n",
      "  1.14384867e-01 -7.20114037e-02 -2.68910676e-02 -3.34956311e-02\n",
      "  7.69403821e-04  4.23742533e-02  7.58477598e-02  1.01838913e-02\n",
      " -4.08687629e-02  3.08614578e-02 -3.51240896e-02  4.65642370e-04\n",
      " -5.88225275e-02  3.62134948e-02 -2.57341843e-02 -3.37240957e-02\n",
      "  9.19845700e-02  1.41429370e-02  6.47979453e-02 -2.04527862e-02\n",
      " -9.55838617e-03  2.10397076e-02  2.55576652e-02  2.91625243e-02\n",
      " -5.30827232e-02  1.27959058e-01  2.21437514e-02 -3.61140627e-33\n",
      "  7.85055831e-02  1.02878893e-02 -5.59065156e-02 -2.75773443e-02\n",
      " -8.07158127e-02  5.97694926e-02 -3.34177613e-02 -8.69846530e-03\n",
      " -4.26150933e-02  5.96508943e-02 -2.53432728e-02 -3.97392400e-02\n",
      " -1.18922787e-02  1.98814254e-02 -9.93642956e-02 -9.49414745e-02\n",
      " -3.10777710e-03 -2.66807266e-02 -3.55477147e-02  5.39633632e-02\n",
      " -1.20544331e-02  5.99990599e-03 -1.05593622e-01 -4.77432609e-02\n",
      "  5.51709067e-03  1.55211482e-02  5.62998578e-02 -1.06869091e-03\n",
      " -4.32666577e-02 -3.63789499e-02 -5.64949848e-02 -2.79647354e-02\n",
      " -8.83288905e-02 -9.38709918e-03 -2.61729136e-02 -5.76670393e-02\n",
      "  2.31352653e-02  1.08735655e-02  4.96144257e-02  9.08838138e-02\n",
      "  2.09706519e-02  2.36131288e-02  4.95037846e-02 -1.31858205e-02\n",
      " -4.26255725e-02 -6.02845885e-02  2.34034173e-02  5.07833920e-02\n",
      "  1.48183517e-05  3.74816768e-02 -3.62297446e-02 -5.64425737e-02\n",
      " -3.56678106e-02  2.53935363e-02  6.83603734e-02  1.62806734e-02\n",
      " -5.93170747e-02 -2.10411157e-02 -1.37283159e-02 -1.63632166e-02\n",
      "  1.08069710e-01 -9.00459103e-03 -4.02758531e-02  2.81304382e-02\n",
      "  1.13945203e-02 -8.11996982e-02 -2.05392577e-02  5.61745726e-02\n",
      "  6.22442849e-02 -2.82216780e-02 -2.58580316e-02 -1.63458940e-02\n",
      "  1.91755290e-03  1.08703440e-02 -7.06295520e-02  3.27931121e-02\n",
      "  5.18168546e-02  5.49969524e-02 -5.17786928e-02 -7.46058747e-02\n",
      " -3.63296010e-02  1.30345393e-03 -4.79246601e-02  4.39469069e-02\n",
      " -7.93108135e-04  9.14103724e-03  1.18021712e-01  7.50819221e-02\n",
      "  8.34710002e-02 -1.25504276e-02  1.04098685e-01  3.85544747e-02\n",
      "  4.34892513e-02  3.85702178e-02 -1.12377070e-01 -2.92595210e-08\n",
      " -4.26170975e-02 -6.15313686e-02 -4.50449847e-02  2.34082118e-02\n",
      "  6.34679049e-02  6.57868013e-03 -2.22567245e-02 -4.58828174e-02\n",
      " -5.59705384e-02  5.54684661e-02 -1.91303175e-02  4.60288599e-02\n",
      " -9.87369195e-02 -3.07271127e-02  3.68758291e-02  1.66714359e-02\n",
      " -4.19574007e-02  4.90989275e-02 -4.68485523e-03  7.40041137e-02\n",
      " -3.70740448e-03 -2.78315097e-02  2.98423804e-02  1.55232791e-02\n",
      " -4.40386012e-02  5.27911447e-03  2.44373772e-02  6.13791915e-03\n",
      "  8.13471377e-02  4.41582315e-02  8.47681090e-02  1.00032404e-01\n",
      " -1.17944563e-02 -4.29831035e-02  3.28335278e-02  5.09445649e-03\n",
      "  2.83903517e-02  3.00928652e-02  3.53394151e-02  4.35722582e-02\n",
      " -5.68581410e-02 -1.10994354e-01 -2.09662952e-02  2.00644769e-02\n",
      "  8.40043053e-02  1.09687131e-02 -4.24023308e-02  4.20571268e-02\n",
      "  1.45145524e-02 -5.93791306e-02 -4.31822240e-02 -6.49409518e-02\n",
      "  5.69743738e-02 -4.75987513e-03  5.02476410e-04  9.20199975e-02\n",
      " -6.96203113e-03 -9.81762819e-03 -2.23584212e-02 -5.03105633e-02\n",
      " -4.96590547e-02  7.64259100e-02 -3.66476551e-02 -3.67359444e-02]\n",
      "(384,)\n"
     ]
    }
   ],
   "source": [
    "import  numpy as np\n",
    "print(np.array(huggingface_embeddings.embed_query(final_documents[0].page_content)))\n",
    "print(np.array(huggingface_embeddings.embed_query(final_documents[0].page_content)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VectorStore Creation\n",
    "vectorstore=FAISS.from_documents(final_documents[:120],huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.01 Knowledge of signals 16.02 Supply and care of equipment 16.03 Road traffic 16.04 Gateman to observe passing trains 16.05 Channel for flange of wheels 16.06 Defects at level crossings 16.07 Obstructions at level crossings 16.08 Parting of a train 16.09 Trespassing 16.10 Transfer of charge of gate 16.11 Height gauges\n",
      "336xvii 336 337 352 355\n",
      "355\n",
      "364 375 375 375 375 376 376 376 376\n",
      "377\n",
      "377 378 378 379 379 381 381 397 401 401 403\n",
      "404-413\n",
      "404\n",
      "404 404 406 409 410 410 410 412 413 413 413\n",
      "xviii\n",
      "CHAPTER XVII\n"
     ]
    }
   ],
   "source": [
    "## Query using Similarity Search\n",
    "query=\"WHAT IS Train Parting?\"\n",
    "relevant_docments=vectorstore.similarity_search(query)\n",
    "\n",
    "print(relevant_docments[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['FAISS', 'HuggingFaceEmbeddings'] vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002F03B206810> search_kwargs={'k': 3}\n"
     ]
    }
   ],
   "source": [
    "retriever=vectorstore.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3})\n",
    "print(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']=\"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hugging Face Hub is an platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c551c8d8302840c780eac231320ad76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swanu\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\swanu\\.cache\\huggingface\\hub\\models--google--flan-t5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ac9fea8c8342a0bfb83b443ff050ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce837e0f9144a46b91546549fd70a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8153bb2034418288db5b771de29ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "745502da65cd4d348a3b96ab62eae5c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6defd9d2fd45e2a553e04b6da8062b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eaa15c43db24d3995fe470e33cb2c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rails\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load FLAN-T5\n",
    "model_id = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "# Prepare query\n",
    "query = \"What is the protection of trains?\"\n",
    "\n",
    "inputs = tokenizer(query, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "\n",
    "# Decode and print response\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hugging Face models can be run locally through the HuggingFacePipeline class.\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"temperature\": 0, \"max_new_tokens\": 300}\n",
    ")\n",
    "\n",
    "llm = hf \n",
    "llm.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following piece of context to answer the question asked.\n",
    "Please try to provide the answer only based on the context\n",
    "\n",
    "{context}\n",
    "Question:{question}\n",
    "\n",
    "Helpful Answers:\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(template=prompt_template,input_variables=[\"context\",\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievalQA=RetrievalQA.from_chain_type(\n",
    "    llm=hf,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\":prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"DIFFERENCES IN THE\n",
    "UNINSURED RATE BY STATE\n",
    "IN 2022\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Use the following piece of context to answer the question asked.\n",
      "Please try to provide the answer only based on the context\n",
      "\n",
      "comparison of ACS and CPS ASEC measures \n",
      "of health insurance coverage, refer to < www.\n",
      "census.gov/topics/health/health-insurance/\n",
      "guidance.html >.\n",
      "9 Respondents may have more than one \n",
      "health insurance coverage type at the time \n",
      "of interview. As a result, adding the total \n",
      "number of people with private coverage and \n",
      "the total number with public coverage will \n",
      "sum to more than the total number with any \n",
      "coverage.โข From 2021 to 2022, nine states \n",
      "reported increases in private \n",
      "coverage, while seven reported \n",
      "decreases (Appendix Table B-2). \n",
      "DIFFERENCES IN THE \n",
      "UNINSURED RATE BY STATE \n",
      "IN 2022\n",
      "In 2022, uninsured rates at the \n",
      "time of interview ranged across \n",
      "states from a low of 2.4 percent \n",
      "in Massachusetts to a high of 16.6 \n",
      "percent in Texas, compared to the \n",
      "national rate of 8.0 percent.10 Ten \n",
      "of the 15 states with uninsured \n",
      "10 The uninsured rates in the District \n",
      "of Columbia and Massachusetts were not \n",
      "statistically different.rates above the national aver -\n",
      "\n",
      "percent (Appendix Table B-5). \n",
      "Medicaid coverage accounted \n",
      "for a portion of that difference. \n",
      "Medicaid coverage was 22.7 per -\n",
      "cent in the group of states that \n",
      "expanded Medicaid eligibility and \n",
      "18.0 percent in the group of nonex -\n",
      "pansion states.\n",
      "CHANGES IN THE UNINSURED \n",
      "RATE BY STATE FROM 2021 \n",
      "TO 2022\n",
      "From 2021 to 2022, uninsured rates \n",
      "decreased across 27 states, while \n",
      "only Maine had an increase. The \n",
      "uninsured rate in Maine increased \n",
      "from 5.7 percent to 6.6 percent, \n",
      "although it remained below the \n",
      "national average. Maineโs uninsured \n",
      "rate was still below 8.0 percent, \n",
      "21 Douglas Conway and Breauna Branch, \n",
      "โHealth Insurance Coverage Status and Type \n",
      "by Geography: 2019 and 2021,โ 2022, < www.\n",
      "census.gov/content/dam/Census/library/\n",
      "publications/2022/acs/acsbr-013.pdf >.\n",
      "\n",
      "library/publications/2022/acs/acsbr-013.pdf >.\n",
      "39 In 2022, the private coverage rates were \n",
      "not statistically different in North Dakota and \n",
      "Utah.Figure /five.tab/period.tab\n",
      "Percentage of Uninsured People for the /two.tab/five.tab Most Populous Metropolitan \n",
      "Areas/colon.tab /two.tab/zero.tab/two.tab/one.tab and /two.tab/zero.tab/two.tab/two.tab\n",
      "(Civilian, noninstitutionalized population) /uni00A0\n",
      "* Denotes a statistically signi๏ฌcant change between 2021 and 2022 at the 90 percent con๏ฌdence level.\n",
      "Note: For information on con๏ฌdentiality protection, sampling error, nonsampling error, and de๏ฌnitions in the American Community\n",
      "Survey, refer to <https://www2.census.gov/programs-surveys/acs/tech_docs/accuracy/ACS_Accuracy_of_Data_2022.pdf>.\n",
      "Source: U.S. Census Bureau, 2021 and 2022 American Community Survey, 1-year estimates. Boston-Cambridge-Newton/comma.tab MA-NH\n",
      "San Francisco-Oakland-Berkeley/comma.tab CA\n",
      "*Detroit-Warren-Dearborn/comma.tab MI\n",
      "Question:DIFFERENCES IN THE\n",
      "UNINSURED RATE BY STATE\n",
      "IN 2022\n",
      "\n",
      "Helpful Answers:\n",
      " 1.\n",
      " 2.\n",
      " 3.\n",
      " 4.\n",
      " 5.\n",
      " 6.\n",
      " 7.\n",
      " 8.\n",
      " 9.\n",
      " 10.\n",
      " 11.\n",
      " 12.\n",
      " 13.\n",
      " 14.\n",
      " 15.\n",
      " 16.\n",
      " 17.\n",
      " 18.\n",
      " 19.\n",
      " 20.\n",
      " 21.\n",
      " 22.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the QA chain with our query.\n",
    "result = retrievalQA.invoke({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
