{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce7ed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "loader = UnstructuredPDFLoader(\"KRCL G & SR 2020.pdf\", mode=\"elements\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f3b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3574fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a4f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import re\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "def process_legal_documents(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"Process loaded PDF documents into structured legal chunks with reference extraction.\"\"\"\n",
    "    \n",
    "    # Step 1: Group by rule number with enhanced pattern matching\n",
    "    def group_by_rule(docs: List[Document]) -> List[Document]:\n",
    "        # Pattern matches all rule types:\n",
    "        # - S.R.2.24, G.R.3.15\n",
    "        # - Standard rules: 2.03, 3.01\n",
    "        # - Rule 4.05, Article 5.02, §6.01\n",
    "        rule_pattern = re.compile(\n",
    "            r\"^(?:(?:S\\.R\\.|G\\.R\\.|Rule|Article|Section|§)?\\s*)?\"\n",
    "            r\"((?:[SG]\\.R\\.)?\\d{1,2}\\.\\d{2}(?:\\.\\d+)?|\\d{1,2}\\-\\d{2})\", \n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        grouped = []\n",
    "        current_rule = None\n",
    "        current_metadata = {}\n",
    "        buffer = []\n",
    "\n",
    "        for doc in docs:\n",
    "            text = doc.page_content.strip()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            if not current_metadata:\n",
    "                current_metadata = doc.metadata.copy()\n",
    "\n",
    "            match = rule_pattern.match(text)\n",
    "            if match:\n",
    "                if current_rule and buffer:\n",
    "                    grouped.append(create_rule_document(buffer, current_rule, current_metadata))\n",
    "                current_rule = match.group(1)\n",
    "                # Normalize rule format\n",
    "                current_rule = current_rule.replace(' ', '')  # Remove spaces in S.R./G.R.\n",
    "                buffer = [text]\n",
    "                current_metadata = doc.metadata.copy()\n",
    "            else:\n",
    "                buffer.append(text)\n",
    "\n",
    "        if current_rule and buffer:\n",
    "            grouped.append(create_rule_document(buffer, current_rule, current_metadata))\n",
    "\n",
    "        return grouped\n",
    "\n",
    "    def create_rule_document(content: List[str], rule: str, metadata: dict) -> Document:\n",
    "        \"\"\"Create a rule document with proper metadata.\"\"\"\n",
    "        rule_type = \"SR\" if rule.startswith('S.R.') else \\\n",
    "                   \"GR\" if rule.startswith('G.R.') else \"Standard\"\n",
    "        \n",
    "        metadata = metadata.copy()\n",
    "        metadata.update({\n",
    "            \"rule\": rule,\n",
    "            \"rule_type\": rule_type,\n",
    "            \"source\": metadata.get(\"source\", \"KRCL G & SR 2020.pdf\"),\n",
    "            \"document_type\": \"legal_rule\"\n",
    "        })\n",
    "        return Document(\n",
    "            page_content=\"\\n\".join(content),\n",
    "            metadata=metadata\n",
    "        )\n",
    "\n",
    "    # Step 2: Group the documents by rule\n",
    "    grouped_docs = group_by_rule(documents)\n",
    "\n",
    "    # Step 3: Merge small rules intelligently\n",
    "    def merge_small_rules(docs: List[Document], min_length: int = 100) -> List[Document]:\n",
    "        merged = []\n",
    "        previous_doc = None\n",
    "\n",
    "        for i, doc in enumerate(docs):\n",
    "            # Handle first document case\n",
    "            if i == 0 and len(doc.page_content) < min_length:\n",
    "                if len(docs) > 1 and docs[i+1].metadata.get('rule_type') == doc.metadata.get('rule_type'):\n",
    "                    # Merge forward with next document\n",
    "                    docs[i+1].page_content = doc.page_content + \"\\n\\n\" + docs[i+1].page_content\n",
    "                    docs[i+1].metadata[\"combined_rules\"] = (\n",
    "                        doc.metadata[\"rule\"] + \"; \" + \n",
    "                        docs[i+1].metadata.get(\"combined_rules\", docs[i+1].metadata[\"rule\"])\n",
    "                    )\n",
    "                    continue\n",
    "                merged.append(doc)\n",
    "                continue\n",
    "\n",
    "            if previous_doc and len(doc.page_content) < min_length:\n",
    "                # Only merge if same rule type\n",
    "                if previous_doc.metadata.get('rule_type') == doc.metadata.get('rule_type'):\n",
    "                    previous_doc.page_content += \"\\n\\n\" + doc.page_content\n",
    "                    previous_doc.metadata[\"combined_rules\"] = (\n",
    "                        previous_doc.metadata.get(\"combined_rules\", previous_doc.metadata[\"rule\"]) + \n",
    "                        f\"; {doc.metadata['rule']}\"\n",
    "                    )\n",
    "                    continue\n",
    "                else:\n",
    "                    merged.append(previous_doc)\n",
    "                    previous_doc = doc\n",
    "                    continue\n",
    "            else:\n",
    "                if previous_doc:\n",
    "                    merged.append(previous_doc)\n",
    "                previous_doc = doc\n",
    "\n",
    "        if previous_doc:\n",
    "            merged.append(previous_doc)\n",
    "\n",
    "        return merged\n",
    "\n",
    "    merged_docs = merge_small_rules(grouped_docs)\n",
    "\n",
    "    # Step 4: Split long rules with metadata preservation and reference extraction\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=150,\n",
    "        length_function=len,\n",
    "        keep_separator=True\n",
    "    )\n",
    "\n",
    "    final_documents = []\n",
    "    for doc in merged_docs:\n",
    "        try:\n",
    "            splits = splitter.split_documents([doc])\n",
    "            \n",
    "            # Extract references from each split\n",
    "            reference_pattern = re.compile(\n",
    "                r\"\\b(S\\.R\\.\\d{1,2}\\.\\d{2}(?:\\.\\d+)?)\\s+of\\s+(G\\.R\\.\\d{1,2}\\.\\d{2}(?:\\.\\d+)?)\\b\"\n",
    "            )\n",
    "            \n",
    "            for chunk_idx, split in enumerate(splits, 1):\n",
    "                # Extract references\n",
    "                references = reference_pattern.findall(split.page_content)\n",
    "                if references:\n",
    "                    split.metadata[\"references\"] = [f\"{sr} of {gr}\" for sr, gr in references]\n",
    "                \n",
    "                # Merge metadata with priority to split-specific values\n",
    "                split.metadata = {\n",
    "                    **doc.metadata,\n",
    "                    **split.metadata,  # Preserves extracted references\n",
    "                    \"chunk_id\": f\"{doc.metadata['rule']}_chunk_{chunk_idx}\",\n",
    "                    \"total_chunks\": len(splits),\n",
    "                    \"chunk_number\": chunk_idx\n",
    "                }\n",
    "            \n",
    "            final_documents.extend(splits)\n",
    "        except Exception as e:\n",
    "            print(f\"Error splitting rule {doc.metadata['rule']}: {str(e)}\")\n",
    "            doc.metadata.update({\n",
    "                \"chunk_id\": f\"{doc.metadata['rule']}_chunk_1\",\n",
    "                \"total_chunks\": 1,\n",
    "                \"chunk_number\": 1,\n",
    "                \"references\": extract_references(doc.page_content)  # Extract even if not split\n",
    "            })\n",
    "            final_documents.append(doc)\n",
    "\n",
    "    return final_documents\n",
    "\n",
    "def extract_references(text: str) -> List[str]:\n",
    "    \"\"\"Extract cross-references from text.\"\"\"\n",
    "    reference_pattern = re.compile(\n",
    "        r\"\\b(S\\.R\\.\\d{1,2}\\.\\d{2}(?:\\.\\d+)?)\\s+of\\s+(G\\.R\\.\\d{1,2}\\.\\d{2}(?:\\.\\d+)?)\\b\"\n",
    "    )\n",
    "    references = reference_pattern.findall(text)\n",
    "    return [f\"{sr} of {gr}\" for sr, gr in references]\n",
    "\n",
    "# Process documents\n",
    "documents = process_legal_documents(docs)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n✅ Successfully processed {len(processed_docs)} legal document chunks\\n\")\n",
    "print(\"Sample chunks with references:\")\n",
    "for i, doc in enumerate(processed_docs[:5]):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(f\"Rule: {doc.metadata.get('rule')} ({doc.metadata.get('rule_type')})\")\n",
    "    print(f\"Chunk: {doc.metadata.get('chunk_number')}/{doc.metadata.get('total_chunks')}\")\n",
    "    if 'combined_rules' in doc.metadata:\n",
    "        print(f\"Combined with: {doc.metadata['combined_rules']}\")\n",
    "    if 'references' in doc.metadata:\n",
    "        print(f\"References: {', '.join(doc.metadata['references'])}\")\n",
    "    content_preview = doc.page_content[:300] + \"...\" if len(doc.page_content) > 300 else doc.page_content\n",
    "    print(f\"\\n{content_preview}\\n\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c9d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0bf2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9be8364",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Rules of Reporting S & T Gear failures\"\n",
    "retireved_results=db.similarity_search(query)\n",
    "print(retireved_results[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75418f37",
   "metadata": {},
   "source": [
    "## Accidental\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71e07e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "loader = UnstructuredPDFLoader(\"Accident Manaul - 2021_1.pdf\", mode=\"elements\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30207fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 ---\n",
      "Rule: None - None\n",
      "Chapter: Classification Of Accidents\n",
      "Chunk: 1/33\n",
      "KONKAN RAILWAY CORPORATION LIMITED\n",
      "ACCIDENT MANUAL\n",
      "APRIL 2021\n",
      "SAFETY ORGANISATION\n",
      "FOREWORD\n",
      "The Accident Manual is a compendium of all instructions, rules and regulations and guidelines issued from time to time on the subject of Railway Accidents.\n",
      "This New Accident Manual is brought-out after reviewi...\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Rule: None - None\n",
      "Chapter: Classification Of Accidents\n",
      "Chunk: 2/33\n",
      "Accident Manual should be gone through by all the Railway officials, staff who are required to deal with train operations directly or indirectly and those who have to maintain the Railway Assets. All Railway officials should be fully aware and conversant with the provisions of the Accident Manual, G...\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Rule: None - None\n",
      "Chapter: Classification Of Accidents\n",
      "Chunk: 3/33\n",
      "To Railwaymen\n",
      "This Manual brings together in comprehensive manner all the rules, regulation and procedures for dealing with accidents.\n",
      "Last edition of the Accident Manual was published in the year 2012. The present edition contains all the amendments issued since then.\n",
      "Every Railway servant who is r...\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 4 ---\n",
      "Rule: None - None\n",
      "Chapter: Classification Of Accidents\n",
      "Chunk: 4/33\n",
      "This Manual is to be kept upto date by pasting correction slips as and when the same are issued. And should be recorded in the register of amendments/additions along with the particulars pertaining to the rules, date, page No. etc.\n",
      "Suggestions for improving the contents of this Manual should be addr...\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Chunk 5 ---\n",
      "Rule: None - None\n",
      "Chapter: Classification Of Accidents\n",
      "Chunk: 5/33\n",
      "112 Equipment failures\n",
      "113 Unusual incidents\n",
      "114 Reportable Train Accidents\n",
      "115 Collision\n",
      "116 Derailment\n",
      "117 Fire\n",
      "118 Averted Collision\n",
      "119 Breach of Block Rules\n",
      "120 Sabotage\n",
      "121 Train Wrecking\n",
      "i\n",
      "Page No.\n",
      "01\n",
      "01\n",
      "01\n",
      "01\n",
      "01\n",
      "02\n",
      "03\n",
      "03\n",
      "03\n",
      "03\n",
      "03\n",
      "03\n",
      "04\n",
      "04\n",
      "04\n",
      "04\n",
      "04\n",
      "04\n",
      "05\n",
      "05\n",
      "05\n",
      "122 Attempted Train Wrecking\n",
      "123...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Optional\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load PDF\n",
    "loader = UnstructuredPDFLoader(\"Accident Manaul - 2021_1.pdf\", mode=\"elements\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Regex patterns\n",
    "chapter_pattern = re.compile(r\"^CHAPTER\\s*[-–]?\\s*\\d+\\s*(.*)\", re.IGNORECASE)\n",
    "rule_pattern = re.compile(r\"^(\\d{3})\\.\\s+(.+)$\")\n",
    "\n",
    "def create_rule_document(content: List[str], rule: str, title: str, chapter: Optional[str], metadata: dict) -> Document:\n",
    "    \"\"\"Creates a Document from rule section.\"\"\"\n",
    "    metadata = metadata.copy()\n",
    "    metadata.update({\n",
    "        \"rule\": rule,\n",
    "        \"rule_title\": title,\n",
    "        \"chapter\": chapter or \"Unknown\",\n",
    "        \"document_type\": \"accident_manual_rule\",\n",
    "        \"source\": metadata.get(\"source\", \"Accident Manual 2021\")\n",
    "    })\n",
    "    return Document(page_content=\"\\n\".join(content), metadata=metadata)\n",
    "\n",
    "def split_and_merge_chunks(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"Handles merging small chunks and splitting large ones.\"\"\"\n",
    "    final_chunks = []\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)\n",
    "    pending_small = None\n",
    "\n",
    "    for doc in documents:\n",
    "        content = doc.page_content.strip()\n",
    "\n",
    "        if len(content) < 100:\n",
    "            if pending_small:\n",
    "                # Merge small chunks\n",
    "                pending_small.page_content += \"\\n\\n\" + content\n",
    "                pending_small.metadata[\"rule\"] += f\"; {doc.metadata['rule']}\"\n",
    "                pending_small.metadata[\"rule_title\"] += f\"; {doc.metadata['rule_title']}\"\n",
    "            else:\n",
    "                pending_small = doc\n",
    "            continue\n",
    "\n",
    "        elif len(content) > 1000:\n",
    "            # Split large chunk\n",
    "            splits = splitter.split_documents([doc])\n",
    "            for i, split in enumerate(splits, 1):\n",
    "                split.metadata.update({\n",
    "                    \"chunk_id\": f\"{doc.metadata['rule']}_chunk_{i}\",\n",
    "                    \"chunk_number\": i,\n",
    "                    \"total_chunks\": len(splits)\n",
    "                })\n",
    "            final_chunks.extend(splits)\n",
    "        else:\n",
    "            if pending_small:\n",
    "                final_chunks.append(pending_small)\n",
    "                pending_small = None\n",
    "            doc.metadata.update({\n",
    "                \"chunk_id\": f\"{doc.metadata['rule']}_chunk_1\",\n",
    "                \"chunk_number\": 1,\n",
    "                \"total_chunks\": 1\n",
    "            })\n",
    "            final_chunks.append(doc)\n",
    "\n",
    "    if pending_small:\n",
    "        final_chunks.append(pending_small)\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "def process_documents(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"Extract rules and chapters, then chunk intelligently.\"\"\"\n",
    "    current_chapter = None\n",
    "    current_metadata = {}\n",
    "    current_rule = None\n",
    "    current_title = None\n",
    "    buffer = []\n",
    "    grouped_docs = []\n",
    "\n",
    "    for doc in documents:\n",
    "        lines = doc.page_content.splitlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            if chapter_match := chapter_pattern.match(line):\n",
    "                current_chapter = chapter_match.group(1).strip().title()\n",
    "                continue\n",
    "\n",
    "            if rule_match := rule_pattern.match(line):\n",
    "                if buffer:\n",
    "                    grouped_docs.append(create_rule_document(\n",
    "                        buffer, current_rule, current_title, current_chapter, current_metadata))\n",
    "                current_rule = rule_match.group(1)\n",
    "                current_title = rule_match.group(2).strip()\n",
    "                current_metadata = doc.metadata.copy()\n",
    "                buffer = [line]\n",
    "            else:\n",
    "                buffer.append(line)\n",
    "\n",
    "    if buffer:\n",
    "        grouped_docs.append(create_rule_document(\n",
    "            buffer, current_rule, current_title, current_chapter, current_metadata))\n",
    "\n",
    "    return split_and_merge_chunks(grouped_docs)\n",
    "\n",
    "def preview_chunks(processed_docs: List[Document], n=5):\n",
    "    for i, doc in enumerate(processed_docs[:n]):\n",
    "        print(f\"\\n--- Chunk {i+1} ---\")\n",
    "        print(f\"Rule: {doc.metadata.get('rule')} - {doc.metadata.get('rule_title')}\")\n",
    "        print(f\"Chapter: {doc.metadata.get('chapter')}\")\n",
    "        print(f\"Chunk: {doc.metadata.get('chunk_number')}/{doc.metadata.get('total_chunks')}\")\n",
    "        print(doc.page_content[:300] + (\"...\" if len(doc.page_content) > 300 else \"\"))\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# === Main Processing ===\n",
    "processed_docs = process_documents(documents)\n",
    "preview_chunks(processed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba4c9f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swanu\\AppData\\Local\\Temp\\ipykernel_9600\\3623215180.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3653e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(documents, embeddings)\n",
    "db.save_local(\"faiss_manual_krcl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7d5a25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
